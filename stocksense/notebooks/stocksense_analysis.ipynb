{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b308625c",
   "metadata": {},
   "source": [
    "# StockSense: Inventory Intelligence Analysis\n",
    "\n",
    "This notebook demonstrates the complete machine learning workflow for predicting stockouts and optimizing inventory levels.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Data Loading & Exploration](#data-loading)\n",
    "2. [Feature Engineering](#feature-engineering)\n",
    "3. [Model Training](#model-training)\n",
    "4. [Model Evaluation](#model-evaluation)\n",
    "5. [Predictions & Insights](#predictions)\n",
    "6. [Interactive Visualizations](#visualizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9fb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import xgboost as xgb\n",
    "import joblib\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5d8464",
   "metadata": {},
   "source": [
    "## 1. Data Loading & Exploration {#data-loading}\n",
    "\n",
    "Let's start by generating and loading our sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data if it doesn't exist\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('../data')\n",
    "\n",
    "if not os.path.exists('../data/ml_training_data.csv'):\n",
    "    print(\"Generating sample data...\")\n",
    "    from data_generation import main as generate_data\n",
    "    os.chdir('../data')\n",
    "    generate_data()\n",
    "    os.chdir('../notebooks')\n",
    "else:\n",
    "    print(\"Sample data already exists!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e66c0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "df = pd.read_csv('../data/ml_training_data.csv')\n",
    "inventory_df = pd.read_csv('../data/sample_inventory.csv')\n",
    "sales_df = pd.read_csv('../data/sample_sales_history.csv')\n",
    "\n",
    "print(f\"üìä Training Dataset Shape: {df.shape}\")\n",
    "print(f\"üì¶ Inventory Dataset Shape: {inventory_df.shape}\")\n",
    "print(f\"üí∞ Sales Dataset Shape: {sales_df.shape}\")\n",
    "\n",
    "# Display basic info\n",
    "print(\"\\nüîç Dataset Overview:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f513c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data exploration\n",
    "print(\"üìã Dataset Information:\")\n",
    "print(df.info())\n",
    "\n",
    "print(\"\\nüìä Statistical Summary:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043be0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check target variable distribution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Target distribution\n",
    "df['is_high_risk'].value_counts().plot(kind='bar', ax=axes[0], color=['lightgreen', 'salmon'])\n",
    "axes[0].set_title('High Risk Products Distribution')\n",
    "axes[0].set_xlabel('Risk Level (0: Low Risk, 1: High Risk)')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Category distribution\n",
    "df['category'].value_counts().plot(kind='bar', ax=axes[1])\n",
    "axes[1].set_title('Products by Category')\n",
    "axes[1].set_xlabel('Category')\n",
    "axes[1].set_ylabel('Count')\n",
    "axes[1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è  High Risk Products: {df['is_high_risk'].sum()} ({df['is_high_risk'].mean()*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1eaeac",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering {#feature-engineering}\n",
    "\n",
    "Let's create meaningful features for our prediction model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45ad4e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering\n",
    "def engineer_features(data):\n",
    "    \"\"\"Create new features for better prediction\"\"\"\n",
    "    df_features = data.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    df_features['demand_variability'] = df_features['demand_variability'].fillna(0)\n",
    "    \n",
    "    # Price categories\n",
    "    df_features['price_category'] = pd.cut(df_features['price'], \n",
    "                                          bins=[0, 20, 100, 500, float('inf')],\n",
    "                                          labels=['Low', 'Medium', 'High', 'Premium'])\n",
    "    \n",
    "    # Demand categories\n",
    "    df_features['demand_category'] = pd.cut(df_features['avg_daily_demand'],\n",
    "                                           bins=[0, 10, 50, 100, float('inf')],\n",
    "                                           labels=['Low', 'Medium', 'High', 'Very High'])\n",
    "    \n",
    "    # Risk indicators\n",
    "    df_features['stockout_rate'] = df_features['total_stockouts'] / 365\n",
    "    df_features['is_fast_moving'] = (df_features['avg_daily_demand'] > df_features['avg_daily_demand'].median()).astype(int)\n",
    "    df_features['lead_time_risk'] = (df_features['supplier_lead_time'] > 7).astype(int)\n",
    "    \n",
    "    # Seasonal indicators\n",
    "    df_features['is_seasonal'] = (df_features['seasonal_factor'] > 1.5).astype(int)\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply feature engineering\n",
    "df_engineered = engineer_features(df)\n",
    "print(f\"‚úÖ Feature engineering completed! New shape: {df_engineered.shape}\")\n",
    "print(f\"üìä New features added: {df_engineered.shape[1] - df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7b537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "numerical_cols = df_engineered.select_dtypes(include=[np.number]).columns\n",
    "corr_matrix = df_engineered[numerical_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Show correlation with target\n",
    "target_corr = corr_matrix['is_high_risk'].sort_values(key=abs, ascending=False)\n",
    "print(\"\\nüéØ Correlation with High Risk Target:\")\n",
    "print(target_corr.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83956b2e",
   "metadata": {},
   "source": [
    "## 3. Model Training {#model-training}\n",
    "\n",
    "Let's train multiple models and compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9aa033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "def prepare_model_data(data):\n",
    "    \"\"\"Prepare features and target for modeling\"\"\"\n",
    "    df_model = data.copy()\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le_category = LabelEncoder()\n",
    "    le_subcategory = LabelEncoder()\n",
    "    le_price_cat = LabelEncoder()\n",
    "    le_demand_cat = LabelEncoder()\n",
    "    \n",
    "    df_model['category_encoded'] = le_category.fit_transform(df_model['category'])\n",
    "    df_model['subcategory_encoded'] = le_subcategory.fit_transform(df_model['subcategory'])\n",
    "    df_model['price_category_encoded'] = le_price_cat.fit_transform(df_model['price_category'])\n",
    "    df_model['demand_category_encoded'] = le_demand_cat.fit_transform(df_model['demand_category'])\n",
    "    \n",
    "    # Select features for modeling\n",
    "    feature_cols = [\n",
    "        'price', 'supplier_lead_time', 'minimum_stock_level', 'seasonal_factor',\n",
    "        'avg_daily_demand', 'demand_std', 'max_daily_demand', 'total_stockouts',\n",
    "        'weekend_sales_ratio', 'holiday_sales_ratio', 'current_stock',\n",
    "        'days_since_restock', 'demand_variability', 'stock_coverage_days',\n",
    "        'category_encoded', 'subcategory_encoded', 'price_category_encoded',\n",
    "        'demand_category_encoded', 'stockout_rate', 'is_fast_moving',\n",
    "        'lead_time_risk', 'is_seasonal'\n",
    "    ]\n",
    "    \n",
    "    X = df_model[feature_cols]\n",
    "    y = df_model['is_high_risk']\n",
    "    \n",
    "    return X, y, feature_cols\n",
    "\n",
    "# Prepare the data\n",
    "X, y, feature_names = prepare_model_data(df_engineered)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"‚úÖ Data prepared for modeling\")\n",
    "print(f\"üìä Features: {len(feature_names)}\")\n",
    "print(f\"üéØ Training samples: {len(X_train)}\")\n",
    "print(f\"üß™ Test samples: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c1f14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple models\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "# 1. Logistic Regression\n",
    "print(\"üîÑ Training Logistic Regression...\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "lr_pred = lr_model.predict(X_test_scaled)\n",
    "lr_prob = lr_model.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "models['Logistic Regression'] = {'model': lr_model, 'scaler': scaler}\n",
    "results['Logistic Regression'] = {\n",
    "    'predictions': lr_pred,\n",
    "    'probabilities': lr_prob,\n",
    "    'auc': roc_auc_score(y_test, lr_prob)\n",
    "}\n",
    "\n",
    "# 2. Random Forest\n",
    "print(\"üîÑ Training Random Forest...\")\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_pred = rf_model.predict(X_test)\n",
    "rf_prob = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "models['Random Forest'] = {'model': rf_model}\n",
    "results['Random Forest'] = {\n",
    "    'predictions': rf_pred,\n",
    "    'probabilities': rf_prob,\n",
    "    'auc': roc_auc_score(y_test, rf_prob)\n",
    "}\n",
    "\n",
    "# 3. XGBoost\n",
    "print(\"üîÑ Training XGBoost...\")\n",
    "xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "xgb_pred = xgb_model.predict(X_test)\n",
    "xgb_prob = xgb_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "models['XGBoost'] = {'model': xgb_model}\n",
    "results['XGBoost'] = {\n",
    "    'predictions': xgb_pred,\n",
    "    'probabilities': xgb_prob,\n",
    "    'auc': roc_auc_score(y_test, xgb_prob)\n",
    "}\n",
    "\n",
    "print(\"‚úÖ All models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405b1bf4",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation {#model-evaluation}\n",
    "\n",
    "Let's compare the performance of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b29ac33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "print(\"üìä Model Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name, result in results.items():\n",
    "    print(f\"\\nü§ñ {model_name}:\")\n",
    "    print(f\"   AUC Score: {result['auc']:.4f}\")\n",
    "    print(\"\\n   Classification Report:\")\n",
    "    print(classification_report(y_test, result['predictions'], target_names=['Low Risk', 'High Risk']))\n",
    "\n",
    "# AUC comparison\n",
    "auc_scores = {name: result['auc'] for name, result in results.items()}\n",
    "best_model = max(auc_scores, key=auc_scores.get)\n",
    "print(f\"\\nüèÜ Best Model: {best_model} (AUC: {auc_scores[best_model]:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4883e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance for Random Forest\n",
    "if 'Random Forest' in models:\n",
    "    rf_importances = models['Random Forest']['model'].feature_importances_\n",
    "    feature_importance_df = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': rf_importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.barplot(data=feature_importance_df.head(15), x='importance', y='feature')\n",
    "    plt.title('Top 15 Feature Importances (Random Forest)')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nüéØ Top 10 Most Important Features:\")\n",
    "    for i, (_, row) in enumerate(feature_importance_df.head(10).iterrows()):\n",
    "        print(f\"{i+1:2d}. {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c17e64d9",
   "metadata": {},
   "source": [
    "## 5. Predictions & Insights {#predictions}\n",
    "\n",
    "Let's make predictions on our data and generate insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c613583a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the full dataset\n",
    "best_model_obj = models[best_model]['model']\n",
    "\n",
    "if 'scaler' in models[best_model]:\n",
    "    X_scaled = models[best_model]['scaler'].transform(X)\n",
    "    predictions = best_model_obj.predict(X_scaled)\n",
    "    probabilities = best_model_obj.predict_proba(X_scaled)[:, 1]\n",
    "else:\n",
    "    predictions = best_model_obj.predict(X)\n",
    "    probabilities = best_model_obj.predict_proba(X)[:, 1]\n",
    "\n",
    "# Add predictions to the dataframe\n",
    "df_with_predictions = df_engineered.copy()\n",
    "df_with_predictions['predicted_risk'] = predictions\n",
    "df_with_predictions['risk_probability'] = probabilities\n",
    "df_with_predictions['risk_category'] = pd.cut(probabilities, \n",
    "                                             bins=[0, 0.3, 0.7, 1.0],\n",
    "                                             labels=['Low Risk', 'Medium Risk', 'High Risk'])\n",
    "\n",
    "print(\"‚úÖ Predictions completed!\")\n",
    "print(f\"üéØ High Risk Products Identified: {predictions.sum()}\")\n",
    "print(f\"üìä Risk Distribution:\")\n",
    "print(df_with_predictions['risk_category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6210aad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top high-risk products\n",
    "high_risk_products = df_with_predictions[df_with_predictions['predicted_risk'] == 1].sort_values(\n",
    "    'risk_probability', ascending=False\n",
    ")\n",
    "\n",
    "print(\"‚ö†Ô∏è  TOP 10 HIGH RISK PRODUCTS:\")\n",
    "print(\"=\" * 80)\n",
    "for idx, (_, product) in enumerate(high_risk_products.head(10).iterrows()):\n",
    "    print(f\"{idx+1:2d}. {product['product_name']} ({product['category']})\")\n",
    "    print(f\"    Risk Probability: {product['risk_probability']:.2%}\")\n",
    "    print(f\"    Current Stock: {product['current_stock']} units\")\n",
    "    print(f\"    Avg Daily Demand: {product['avg_daily_demand']:.1f} units\")\n",
    "    print(f\"    Stock Coverage: {product['stock_coverage_days']:.1f} days\")\n",
    "    print()\n",
    "\n",
    "# Save predictions\n",
    "output_cols = ['product_id', 'product_name', 'category', 'current_stock', \n",
    "               'avg_daily_demand', 'stock_coverage_days', 'predicted_risk', \n",
    "               'risk_probability', 'risk_category']\n",
    "\n",
    "predictions_output = df_with_predictions[output_cols].copy()\n",
    "predictions_output.to_csv('../outputs/predictions.csv', index=False)\n",
    "print(\"üíæ Predictions saved to ../outputs/predictions.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c468dc3",
   "metadata": {},
   "source": [
    "## 6. Interactive Visualizations {#visualizations}\n",
    "\n",
    "Let's create interactive visualizations to better understand our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0558867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive risk distribution by category\n",
    "risk_by_category = df_with_predictions.groupby(['category', 'risk_category']).size().reset_index(name='count')\n",
    "\n",
    "fig = px.bar(risk_by_category, x='category', y='count', color='risk_category',\n",
    "             title='Risk Distribution by Product Category',\n",
    "             color_discrete_map={'Low Risk': 'green', 'Medium Risk': 'orange', 'High Risk': 'red'})\n",
    "fig.update_layout(height=500)\n",
    "fig.show()\n",
    "\n",
    "print(\"üìä Risk distribution by category visualized above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599232a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot: Stock coverage vs Risk probability\n",
    "fig = px.scatter(df_with_predictions, \n",
    "                x='stock_coverage_days', \n",
    "                y='risk_probability',\n",
    "                color='category',\n",
    "                size='avg_daily_demand',\n",
    "                hover_data=['product_name', 'current_stock', 'supplier_lead_time'],\n",
    "                title='Stock Coverage vs Risk Probability',\n",
    "                labels={'stock_coverage_days': 'Stock Coverage (Days)', \n",
    "                       'risk_probability': 'Risk Probability'})\n",
    "fig.add_hline(y=0.5, line_dash=\"dash\", line_color=\"red\", \n",
    "              annotation_text=\"High Risk Threshold\")\n",
    "fig.update_layout(height=600)\n",
    "fig.show()\n",
    "\n",
    "print(\"üìà Stock coverage vs risk probability scatter plot above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670c4213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model performance dashboard\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Model AUC Comparison', 'Actual vs Predicted', \n",
    "                   'Risk Distribution', 'Top Risk Factors'),\n",
    "    specs=[[{\"type\": \"bar\"}, {\"type\": \"bar\"}],\n",
    "           [{\"type\": \"pie\"}, {\"type\": \"bar\"}]]\n",
    ")\n",
    "\n",
    "# AUC comparison\n",
    "model_names = list(auc_scores.keys())\n",
    "auc_values = list(auc_scores.values())\n",
    "fig.add_trace(\n",
    "    go.Bar(x=model_names, y=auc_values, name=\"AUC Score\", \n",
    "           marker_color=['gold' if name == best_model else 'lightblue' for name in model_names]),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Actual vs Predicted\n",
    "confusion = pd.crosstab(df_with_predictions['is_high_risk'], df_with_predictions['predicted_risk'])\n",
    "fig.add_trace(\n",
    "    go.Bar(x=['True Low Risk', 'False High Risk', 'False Low Risk', 'True High Risk'], \n",
    "           y=[confusion.iloc[0,0], confusion.iloc[0,1], confusion.iloc[1,0], confusion.iloc[1,1]],\n",
    "           name=\"Predictions\",\n",
    "           marker_color=['green', 'orange', 'orange', 'red']),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Risk distribution pie chart\n",
    "risk_dist = df_with_predictions['risk_category'].value_counts()\n",
    "fig.add_trace(\n",
    "    go.Pie(labels=risk_dist.index, values=risk_dist.values, name=\"Risk Distribution\"),\n",
    "    row=2, col=1\n",
    ")\n",
    "\n",
    "# Top risk factors\n",
    "if 'Random Forest' in models:\n",
    "    top_features = feature_importance_df.head(8)\n",
    "    fig.add_trace(\n",
    "        go.Bar(x=top_features['importance'], y=top_features['feature'], \n",
    "               orientation='h', name=\"Importance\"),\n",
    "        row=2, col=2\n",
    "    )\n",
    "\n",
    "fig.update_layout(height=800, title_text=\"StockSense Model Performance Dashboard\", showlegend=False)\n",
    "fig.show()\n",
    "\n",
    "print(\"üìä Comprehensive performance dashboard displayed above\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d55efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "model_data = {\n",
    "    'model': best_model_obj,\n",
    "    'feature_names': feature_names,\n",
    "    'model_type': best_model,\n",
    "    'performance': {\n",
    "        'auc_score': auc_scores[best_model],\n",
    "        'test_accuracy': (results[best_model]['predictions'] == y_test).mean()\n",
    "    }\n",
    "}\n",
    "\n",
    "if 'scaler' in models[best_model]:\n",
    "    model_data['scaler'] = models[best_model]['scaler']\n",
    "\n",
    "joblib.dump(model_data, '../models/stockout_model.pkl')\n",
    "\n",
    "print(f\"‚úÖ Best model ({best_model}) saved to ../models/stockout_model.pkl\")\n",
    "print(f\"üìà Final AUC Score: {auc_scores[best_model]:.4f}\")\n",
    "print(\"\\nüéØ StockSense Analysis Complete!\")\n",
    "print(\"üìã Summary:\")\n",
    "print(f\"   - Best Model: {best_model}\")\n",
    "print(f\"   - AUC Score: {auc_scores[best_model]:.4f}\")\n",
    "print(f\"   - High Risk Products: {predictions.sum()}\")\n",
    "print(f\"   - Model saved and ready for deployment!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
